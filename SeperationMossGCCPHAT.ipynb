{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "#BEFORE YOU RUN THIS FILE, YOU SHOULD RUN THE CLEANING ARCHETECTURE FIRST FOR THE FILE AUDIOS, CHECK THE OTHER FILE.\n",
        "#SET UP for MossFormer\n",
        "if os.path.exists('checkpoints'):\n",
        "    shutil.rmtree('checkpoints')\n",
        "\n",
        "!pip install clearvoice\n",
        "import torch\n",
        "from clearvoice import ClearVoice\n",
        "import numpy as np\n",
        "import librosa\n",
        "import soundfile as sf\n",
        "from IPython.display import Audio, display\n",
        "\n",
        "# 2. Initialize Engine (This will now re-download correctly)\n",
        "print(\"Initializing MossFormer2 Engine (Best of Both Worlds)...\")\n",
        "mv_engine = ClearVoice(task='speech_separation', model_names=['MossFormer2_SS_16K'])\n",
        "#After this runs you will need to restart collab session"
      ],
      "metadata": {
        "collapsed": true,
        "id": "TWGu1AieI0Fz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import os\n",
        "import torch\n",
        "import torchaudio\n",
        "import numpy as np\n",
        "import scipy.signal as signal\n",
        "import gc\n",
        "from google.colab import drive\n",
        "\n",
        "# 1. Mount Drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 2. Path Setup (Change File name here)\n",
        "session_path = \"/content/drive/My Drive/GEEKED/Audio_Files/Session_008\"\n",
        "\n",
        "doc_path = os.path.join(session_path, \"doc_clean.wav\")\n",
        "pat_path = os.path.join(session_path, \"pat_clean.wav\")\n",
        "import librosa\n",
        "import numpy as np\n",
        "\n",
        "# 1. Load both wav files into numpy arrays, .wavs should be in 16k\n",
        "doctor_audio, sr1 = librosa.load(doc_path, sr=16000)\n",
        "patient_audio, sr2 = librosa.load(pat_path, sr=16000)\n",
        "\n",
        "# 2. Ensure they are the exact same length to avoid math errors\n",
        "min_length = min(len(doctor_audio), len(patient_audio))\n",
        "doctor_audio = doctor_audio[:min_length]\n",
        "patient_audio = patient_audio[:min_length]\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "LP5wHiD7RKk7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "0lVATWrn1aWx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import scipy.signal as signal\n",
        "#Does the gcc phase transform calculations, essentially shows us information about phase differences\n",
        "def calculate_gcc_phat(mic1_chunk, mic2_chunk, sample_rate=16000, max_tau=30, interp=16):\n",
        "    window = np.hanning(len(mic1_chunk))\n",
        "    c1, c2 = mic1_chunk * window, mic2_chunk * window\n",
        "    n = len(mic1_chunk) + len(mic2_chunk)\n",
        "    STFT1, STFT2 = np.fft.rfft(c1, n=n), np.fft.rfft(c2, n=n)\n",
        "    freqs = np.fft.rfftfreq(n, d=1.0/sample_rate)\n",
        "    voice_band = (freqs > 300) & (freqs < 3400)\n",
        "    R = STFT1 * np.conj(STFT2)\n",
        "    phat_weighting = np.zeros_like(R)\n",
        "    phat_weighting[voice_band] = R[voice_band] / (np.abs(R[voice_band]) + 1e-10)\n",
        "    cc = np.fft.irfft(phat_weighting, n=(interp * n))\n",
        "    center = int(interp * n / 2)\n",
        "    cc = np.concatenate((cc[-center:], cc[:center+1]))\n",
        "    bound = int(interp * max_tau)\n",
        "    cc = cc[center - bound : center + bound + 1]\n",
        "    peak_index = np.argmax(np.abs(cc))\n",
        "    tau = (peak_index - bound) / float(interp)\n",
        "    return tau, np.abs(cc[peak_index]) / (np.mean(np.abs(cc)) + 1e-10)"
      ],
      "metadata": {
        "id": "4aio58iqRRDg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "#Plots gcc phat info\n",
        "def plot_raw_spatial_delays(doc_audio, pat_audio, start_sec=0, end_sec=280, sr=16000, window_ms=30):\n",
        "    window_samples = int((window_ms / 1000) * sr)\n",
        "    num_windows = int((end_sec - start_sec) * sr // window_samples)\n",
        "\n",
        "    times, taus = [], []\n",
        "\n",
        "    print(f\"üïµÔ∏è Mapping raw spatial dots for {num_windows} windows...\")\n",
        "\n",
        "    for i in range(num_windows):\n",
        "        s = int(start_sec * sr) + (i * window_samples)\n",
        "        e = s + window_samples\n",
        "        if e > len(doc_audio): break\n",
        "\n",
        "        c1, c2 = doc_audio[s:e], pat_audio[s:e]\n",
        "\n",
        "        # Max-normalize the chunk to help the radar see quiet voices\n",
        "        m = max(np.max(np.abs(c1)), np.max(np.abs(c2)))\n",
        "\n",
        "        if m > 1e-5:\n",
        "            # GCC-PHAT function to get the raw Tau\n",
        "            tau, _ = calculate_gcc_phat(c1/m, c2/m, max_tau=30, interp=16)\n",
        "\n",
        "            times.append(start_sec + (i * window_ms / 1000))\n",
        "            taus.append(tau)\n",
        "\n",
        "    # --- SINGLE PLOT: RAW SPATIAL DELAYS ---\n",
        "    plt.figure(figsize=(16, 8))\n",
        "\n",
        "    # We keep the exact styling from Plot 2: Blue dots, small size, subtle alpha\n",
        "    plt.scatter(times, taus, c='blue', s=15, alpha=0.8, edgecolors='none')\n",
        "\n",
        "    # Keeping the horizontal center line for reference\n",
        "    plt.axhline(0, color='black', linestyle='--', alpha=0.3, label='Center')\n",
        "\n",
        "    plt.title(\"Raw Spatial Delays (GCC-PHAT Tau)\", fontsize=14)\n",
        "    plt.ylabel(\"Sample Delay (Tau)\")\n",
        "    plt.xlabel(\"Time (Seconds)\")\n",
        "    plt.ylim(-35, 35) # Matching your previous range\n",
        "\n",
        "    plt.xlim(start_sec, end_sec)\n",
        "    plt.grid(True, alpha=0.1) # Light grid for better readability\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Run it\n",
        "plot_raw_spatial_delays(doctor_audio, patient_audio)"
      ],
      "metadata": {
        "id": "EUWhqf_wSx9I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.neighbors import KDTree\n",
        "#trims the graph to find the denser clusters and get rid of noise\n",
        "def find_dense_clusters(doc_audio, pat_audio, sr=16000, window_ms=30):\n",
        "    window_samples = int((window_ms / 1000) * sr)\n",
        "    num_windows = len(doc_audio) // window_samples\n",
        "\n",
        "    raw_points = []\n",
        "    print(\"Gathering raw spatial data...\")\n",
        "\n",
        "    # 1. Collect all points where there is clear audio\n",
        "    for i in range(num_windows):\n",
        "        s, e = i * window_samples, (i+1) * window_samples\n",
        "        if np.max(np.abs(doc_audio[s:e])) > 0.0006:\n",
        "            tau, _ = calculate_gcc_phat(doc_audio[s:e], pat_audio[s:e], max_tau=30)\n",
        "            raw_points.append([i * (window_ms/1000), tau])\n",
        "\n",
        "    points = np.array(raw_points)\n",
        "\n",
        "    # 2. Density Filtering (The \"Loner\" Filter)\n",
        "    # We look at every dot and ask: \"Are there at least 5 dots within a small radius?\"\n",
        "    # This kills the 'jumps' and the noise instantly.\n",
        "    print(\"Filtering out the noise...\")\n",
        "    tree = KDTree(points)\n",
        "    # Radius of 2 seconds and 2 samples of delay\n",
        "    count = tree.query_radius(points, r=2.0, count_only=True)\n",
        "\n",
        "    # Keep only points in dense 'lanes'\n",
        "    dense_points = points[count > 10]\n",
        "\n",
        "    return points, dense_points\n",
        "\n",
        "# --- RUN THE CLUSTER FINDER ---\n",
        "raw_all, raw_dense = find_dense_clusters(doctor_audio, patient_audio)\n",
        "def visualize_clusters(all_points, dense_points):\n",
        "    plt.figure(figsize=(16, 8))\n",
        "\n",
        "    # Plot the raw \"Mess\" in light grey\n",
        "    plt.scatter(all_points[:, 0], all_points[:, 1], color='lightgrey', s=2, alpha=0.3, label=\"Raw Math (The Mess)\")\n",
        "\n",
        "    # Plot the \"Dense Clusters\" (The real voices) in Black\n",
        "    plt.scatter(dense_points[:, 0], dense_points[:, 1], color='black', s=5, alpha=0.6, label=\"Dense Clusters (The Voices)\")\n",
        "\n",
        "    plt.axhline(0, color='blue', linestyle='--', alpha=0.2, label=\"Perfect Alignment\")\n",
        "    plt.title(\"Acoustic Fingerprint: Identifying the Two Main Lanes\", fontsize=16)\n",
        "    plt.xlabel(\"Time (Seconds)\")\n",
        "    plt.ylabel(\"Sample Delay\")\n",
        "    plt.ylim(-30, 30)\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.1)\n",
        "    plt.show()\n",
        "\n",
        "visualize_clusters(raw_all, raw_dense)"
      ],
      "metadata": {
        "id": "QkczXE76elol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "#my human-based RL function (I was the agent and policy)\n",
        "def run_ultimate_robust_radar(dense_points):\n",
        "    pts = dense_points[dense_points[:, 0].argsort()]\n",
        "    lanes = []\n",
        "    locked = False\n",
        "    labeled_data = []\n",
        "\n",
        "    # --- TUNING PARAMETERS ---\n",
        "    momentum = 0.96        # Base momentum\n",
        "    max_step = 1.2         # Drift speed\n",
        "    shield_limit = 23.0    # THE FIX: No dot allowed > 23 samples from a lane\n",
        "    doubt_width = 4.0      # The \"Iffy\" middle zone\n",
        "\n",
        "    last_t = 0\n",
        "    upper_idx = None # Geometric lock (Upper vs Lower)\n",
        "\n",
        "    print(\"Radar Active: Integrated Shielding, Doubt-Zones, and Re-Acquisition...\")\n",
        "\n",
        "    for i, (t, tau) in enumerate(pts):\n",
        "        dt = t - last_t\n",
        "\n",
        "        # 1. LOCAL DENSITY VETTING (Filter outliers)\n",
        "        start_idx = max(0, i-15); end_idx = min(len(pts), i+15)\n",
        "        neighbors = pts[start_idx:end_idx]\n",
        "        friend_count = sum(1 for nt, ntau in neighbors if abs(ntau - tau) < 3.0 and abs(nt - t) < 1.0)\n",
        "\n",
        "        if friend_count < 4:\n",
        "            labeled_data.append([t, tau, lanes[0] if len(lanes)>0 else tau,\n",
        "                                 lanes[1] if len(lanes)>1 else None, \"Noise\"])\n",
        "            continue\n",
        "\n",
        "        # 2. DISCOVERY\n",
        "        if len(lanes) == 0:\n",
        "            lanes.append(tau)\n",
        "            labeled_data.append([t, tau, tau, None, \"L1\"])\n",
        "            continue\n",
        "\n",
        "        # 3. DUAL-LANE TRACKING (The \"Shielded & Territorial\" Mode)\n",
        "        if locked:\n",
        "            if upper_idx is None:\n",
        "                upper_idx = 0 if lanes[0] > lanes[1] else 1\n",
        "            lower_idx = 1 if upper_idx == 0 else 0\n",
        "\n",
        "            # --- SHIELDING & SKEPTICISM ---\n",
        "            # Reject if dot is physically impossible relative to current lanes\n",
        "            is_outlier = (tau > lanes[upper_idx] + shield_limit) or (tau < lanes[lower_idx] - shield_limit)\n",
        "            if is_outlier:\n",
        "                labeled_data.append([t, tau, lanes[0], lanes[1], \"Noise\"])\n",
        "                continue\n",
        "\n",
        "            # Identify target lane based on territory (Midpoint Fence)\n",
        "            midpoint = (lanes[0] + lanes[1]) / 2\n",
        "            target, other = (0, 1) if abs(tau - lanes[0]) < abs(tau - lanes[1]) else (1, 0)\n",
        "\n",
        "            # --- PROBABILISTIC DOUBT ZONE ---\n",
        "            # If dot is in the middle, increase momentum (become stubborn)\n",
        "            dist_to_mid = abs(tau - midpoint)\n",
        "            active_momentum = 0.999 if dist_to_mid < doubt_width else momentum\n",
        "\n",
        "            # --- SEARCH & SNAP (SESSION 9 FIX) ---\n",
        "            # If we haven't seen dots in a while and this is a HIGH DENSITY cluster\n",
        "            is_global_jump = (dt > 2.0 and friend_count > 12 and abs(tau - lanes[target]) > 8.0)\n",
        "\n",
        "            # TERRITORIAL PROTECTION: No crossing the midpoint!\n",
        "            is_illegal_cross = (target == upper_idx and tau < midpoint) or (target == lower_idx and tau > midpoint)\n",
        "\n",
        "            if not is_illegal_cross:\n",
        "                if is_global_jump:\n",
        "                    # SNAP the rig to the new cluster location\n",
        "                    shift = tau - lanes[target]\n",
        "                    lanes[target] = tau\n",
        "                    lanes[other] += (shift * 0.8) # Loose tether nudge\n",
        "                else:\n",
        "                    # NORMAL TRACKING\n",
        "                    shift = np.clip((tau - lanes[target]) * (1 - active_momentum), -max_step, max_step)\n",
        "                    lanes[target] += shift\n",
        "                    lanes[other] += (shift * 0.5) # The \"Smaller Nudge\" logic\n",
        "\n",
        "                last_t = t\n",
        "                labeled_data.append([t, tau, lanes[0], lanes[1], \"L1\" if target == 0 else \"L2\"])\n",
        "            else:\n",
        "                labeled_data.append([t, tau, lanes[0], lanes[1], \"Noise\"])\n",
        "\n",
        "        # 4. DISCOVERY MODE (FINDING B)\n",
        "        else:\n",
        "            if abs(tau - lanes[0]) < 5.0:\n",
        "                lanes[0] = lanes[0] * momentum + tau * (1 - momentum)\n",
        "                labeled_data.append([t, tau, lanes[0], None, \"L1\"])\n",
        "            elif 12.0 < abs(tau - lanes[0]) < 30.0:\n",
        "                # Validation cluster\n",
        "                if friend_count > 10:\n",
        "                    lanes.append(tau)\n",
        "                    locked = True\n",
        "                    labeled_data.append([t, tau, lanes[0], lanes[1], \"L2\"])\n",
        "                else:\n",
        "                    labeled_data.append([t, tau, lanes[0], None, \"Finding\"])\n",
        "\n",
        "    # --- FINAL IDENTITY MAPPING ---\n",
        "    res = np.array(labeled_data, dtype=object)\n",
        "    l1_m = np.mean(res[:, 2].astype(float))\n",
        "    l2_vals = [x for x in res[:, 3] if x is not None]\n",
        "    l2_m = np.mean(l2_vals) if l2_vals else 100\n",
        "    mapping = {\"L1\": \"Doctor\", \"L2\": \"Patient\"} if abs(l1_m) < abs(l2_m) else {\"L1\": \"Patient\", \"L2\": \"Doctor\"}\n",
        "    for row in res:\n",
        "        row[4] = mapping.get(row[4], row[4])\n",
        "    return res"
      ],
      "metadata": {
        "id": "NZ8eJ8iNfo2m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# see the radar algo at work\n",
        "def visualize_warm_start(res, all_raw):\n",
        "    plt.figure(figsize=(16, 8))\n",
        "    plt.scatter(all_raw[:, 0], all_raw[:, 1], color='lightgrey', s=1, alpha=0.1)\n",
        "\n",
        "    t = res[:, 0].astype(float)\n",
        "    tau = res[:, 1].astype(float)\n",
        "    l1 = res[:, 2].astype(float)\n",
        "    l2 = np.array([float(x) if x is not None else np.nan for x in res[:, 3]])\n",
        "    labels = res[:, 4]\n",
        "\n",
        "    # Plot Finding Phase (Yellow/Orange)\n",
        "    find_mask = (labels == \"Finding_Patient\") | (labels == \"Finding_Doctor\")\n",
        "    plt.scatter(t[find_mask], tau[find_mask], color='orange', s=10, alpha=0.4, label=\"Acquiring Target...\")\n",
        "\n",
        "    # Plot Confirmed Speakers\n",
        "    plt.scatter(t[labels == \"Doctor\"], tau[labels == \"Doctor\"], color='blue', s=10, alpha=0.6, label=\"Doctor\")\n",
        "    plt.scatter(t[labels == \"Patient\"], tau[labels == \"Patient\"], color='red', s=10, alpha=0.6, label=\"Patient\")\n",
        "\n",
        "    # Lines\n",
        "    plt.plot(t, l1, color='blue', linewidth=2, alpha=0.3)\n",
        "    plt.plot(t, l2, color='red', linewidth=2, alpha=0.3)\n",
        "\n",
        "    plt.axhline(0, color='black', linestyle='--', alpha=0.2)\n",
        "    plt.title(\"Warm-Start Radar: Reactive Cluster Recognition\", fontsize=16)\n",
        "    plt.legend(markerscale=2)\n",
        "    plt.show()\n",
        "\n",
        "# --- RUN IT ---\n",
        "warm_results = run_ultimate_robust_radar(raw_dense)\n",
        "visualize_warm_start(warm_results, raw_all)"
      ],
      "metadata": {
        "id": "bUyUZDGqkiYN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from clearvoice import ClearVoice\n",
        "import os\n",
        "#Run Moss to get AI outputs\n",
        "# Initialize the Engine\n",
        "mv_engine = ClearVoice(task='speech_separation', model_names=['MossFormer2_SS_16K'])\n",
        "\n",
        "# Path to the file containing both voices\n",
        "input_path = os.path.join(session_path, \"doc_clean.wav\")\n",
        "\n",
        "print(f\"üöÄ AI is unweaving voices from: {os.path.basename(input_path)}\")\n",
        "\n",
        "# RUN THE ENGINE\n",
        "with torch.no_grad():\n",
        "    m1_sep = mv_engine(input_path=input_path, online_write=False)\n",
        "\n",
        "# Track 0 and Track 1 are now sitting directly in your RAM\n",
        "print(\"‚úÖ AI Separation Complete.\")"
      ],
      "metadata": {
        "id": "y8SPeWA9ZM8R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def calculate_rms(audio_chunk):\n",
        "    return np.sqrt(np.mean(np.square(audio_chunk)) + 1e-10)\n",
        "\n",
        "# This is my rudimentary identification of who is doctor and who is patient based on loudness in segments\n",
        "# If I had more time, I would run WHISPR on every output and see who says \"You\" more and sciency language more like \"Sugar\" \"Blood\" etcetc to see who is doctor\n",
        "def classify_lanes_by_energy(radar_results, doc_audio, sr=16000):\n",
        "    \"\"\"\n",
        "    Identifies which radar lane belongs to the Doctor by matching\n",
        "    spatial hits to the loudest parts of the doc_clean mic.\n",
        "    \"\"\"\n",
        "    hop = int(0.04 * sr)\n",
        "    rms_doc = np.array([np.sqrt(np.mean(doc_audio[i:i+hop]**2))\n",
        "                        for i in range(0, len(doc_audio)-hop, hop)])\n",
        "    # Focus on the top 30% loudest segments to find the primary speaker\n",
        "    threshold = np.percentile(rms_doc, 70)\n",
        "    loud_times = np.where(rms_doc > threshold)[0] * 0.04\n",
        "\n",
        "    l1_hits, l2_hits = 0, 0\n",
        "    for t_loud in loud_times:\n",
        "        idx = np.abs(radar_results[:, 0].astype(float) - t_loud).argmin()\n",
        "        tau = float(radar_results[idx, 1])\n",
        "        l1, l2 = float(radar_results[idx, 2]), float(radar_results[idx, 3])\n",
        "\n",
        "        if abs(tau - l1) < 4.0: l1_hits += 1\n",
        "        if l2 is not None and abs(tau - l2) < 4.0: l2_hits += 1\n",
        "\n",
        "    mapping = {\"L1\": \"Doctor\", \"L2\": \"Patient\"} if l1_hits >= l2_hits else {\"L1\": \"Patient\", \"L2\": \"Doctor\"}\n",
        "    print(f\"üìä Identity Match: L1({l1_hits} hits), L2({l2_hits} hits). Mapping: {mapping}\")\n",
        "    return mapping\n",
        "\n",
        "def event_based_stitcher_with_forensics(radar_truth, clusters_t0, clusters_t1, track_a, track_b, sr=16000, window_sec=0.04):\n",
        "    \"\"\"\n",
        "    v3 Stitcher: Sticky identity, 10ms cross-fades, and gap bridging to stop popping.\n",
        "    \"\"\"\n",
        "    total_samples = len(track_a)\n",
        "    window_samples = int(window_sec * sr)\n",
        "    fade_samples = int(0.010 * sr)\n",
        "\n",
        "    final_doctor = np.zeros_like(track_a)\n",
        "    final_patient = np.zeros_like(track_a)\n",
        "\n",
        "    CORE_RADIUS = 5.0\n",
        "    STICKY_RADIUS = 12.0\n",
        "    GAP_BRIDGE_LIMIT = 4 # Bridge up to 160ms of silence\n",
        "\n",
        "    radar_times = radar_truth[:, 0].astype(float)\n",
        "    l1_vals = radar_truth[:, 2].astype(float)\n",
        "    l2_vals = np.array([float(x) if x is not None else np.nan for x in radar_truth[:, 3]])\n",
        "\n",
        "    decision_log = []\n",
        "    prev_id_a, prev_id_b = None, None\n",
        "    gap_a, gap_b = 0, 0\n",
        "\n",
        "    def get_radar_lanes(t):\n",
        "        idx = np.abs(radar_times - t).argmin()\n",
        "        return l1_vals[idx], l2_vals[idx]\n",
        "\n",
        "    def identify_with_hysteresis(taus, d_lane, p_lane, last_id, gap_count):\n",
        "        if not taus:\n",
        "            if last_id is not None and gap_count < GAP_BRIDGE_LIMIT:\n",
        "                return last_id, gap_count + 1\n",
        "            return None, 0\n",
        "        loc = np.median(taus)\n",
        "        r_doc = STICKY_RADIUS if last_id == \"Doctor\" else CORE_RADIUS\n",
        "        r_pat = STICKY_RADIUS if last_id == \"Patient\" else CORE_RADIUS\n",
        "        if abs(loc - d_lane) < r_doc: return \"Doctor\", 0\n",
        "        if not np.isnan(p_lane) and abs(loc - p_lane) < r_pat: return \"Patient\", 0\n",
        "        return None, 0\n",
        "\n",
        "    fade_in = np.linspace(0, 1, fade_samples)\n",
        "    fade_out = np.linspace(1, 0, fade_samples)\n",
        "\n",
        "    for i in range(0, total_samples - window_samples, window_samples):\n",
        "        s, e = i, i + window_samples\n",
        "        t_now = (s + e) / (2 * sr)\n",
        "        d_lane, p_lane = get_radar_lanes(t_now)\n",
        "\n",
        "        ca, cb = track_a[s:e].copy(), track_b[s:e].copy()\n",
        "        t0_taus = [pt[1] for pt in clusters_t0 if s/sr <= pt[0] < e/sr]\n",
        "        t1_taus = [pt[1] for pt in clusters_t1 if s/sr <= pt[0] < e/sr]\n",
        "\n",
        "        id_a, gap_a = identify_with_hysteresis(t0_taus, d_lane, p_lane, prev_id_a, gap_a)\n",
        "        id_b, gap_b = identify_with_hysteresis(t1_taus, d_lane, p_lane, prev_id_b, gap_b)\n",
        "\n",
        "        # Logging for the plot\n",
        "        if t0_taus: decision_log.append([t_now, np.median(t0_taus), \"TrackA\", id_a])\n",
        "        if t1_taus: decision_log.append([t_now, np.median(t1_taus), \"TrackB\", id_b])\n",
        "\n",
        "        # Apply Fades to prevent popping\n",
        "        if id_a is not None and prev_id_a is None: ca[:fade_samples] *= fade_in\n",
        "        if id_b is not None and prev_id_b is None: cb[:fade_samples] *= fade_in\n",
        "\n",
        "        if id_a == \"Doctor\": final_doctor[s:e] = ca\n",
        "        elif id_a == \"Patient\": final_patient[s:e] = ca\n",
        "        if id_b == \"Doctor\":\n",
        "            if not np.any(final_doctor[s:e]): final_doctor[s:e] = cb\n",
        "        elif id_b == \"Patient\":\n",
        "            if not np.any(final_patient[s:e]): final_patient[s:e] = cb\n",
        "        prev_id_a, prev_id_b = id_a, id_b\n",
        "\n",
        "    return final_doctor, final_patient, np.array(decision_log, dtype=object)"
      ],
      "metadata": {
        "id": "GMap0Wi4Hv1t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from matplotlib.lines import Line2D\n",
        "\n",
        "def plot_live_decisions(start_sec, end_sec, warm_results, forensic_log):\n",
        "    \"\"\"\n",
        "    Overlay plot showing raw AI clusters vs. Stitcher's final routing decisions.\n",
        "    - Raw Clusters = Small Gray dots\n",
        "    - Doctor Decisions = Bold Blue dots\n",
        "    - Patient Decisions = Bold Red dots\n",
        "    \"\"\"\n",
        "    # 1. Extract Raw Clusters (The \"Warm Results\" from Radar)\n",
        "    # Expected format: [[time, tau, weight], ...]\n",
        "    raw_data = np.array(warm_results)\n",
        "    raw_times = raw_data[:, 0].astype(float)\n",
        "    raw_taus = raw_data[:, 1].astype(float)\n",
        "\n",
        "    # 2. Extract Stitcher Decisions (The \"Forensic Log\")\n",
        "    # Expected format: [[time, median_tau, track_id, assigned_label], ...]\n",
        "    log_data = np.array(forensic_log, dtype=object)\n",
        "    log_times = log_data[:, 0].astype(float)\n",
        "    log_taus = log_data[:, 1].astype(float)\n",
        "    log_tracks = log_data[:, 2]\n",
        "    log_labels = log_data[:, 3]\n",
        "\n",
        "    plt.figure(figsize=(18, 9))\n",
        "\n",
        "    # --- LAYER 1: THE RAW CLUSTERS (Background \"Truth\") ---\n",
        "    # These represent EVERY spatial hit the AI found before the stitcher touched them.\n",
        "    plt.scatter(raw_times, raw_taus, c='gray', s=5, alpha=0.15, label='Raw AI Clusters')\n",
        "\n",
        "    # --- LAYER 2: THE STITCHER DECISIONS (Foreground \"Routing\") ---\n",
        "    # Separate by Track A and Track B to see if MossFormer was \"swapping\"\n",
        "\n",
        "    for track_name, marker in [(\"TrackA\", \"o\"), (\"TrackB\", \"x\")]:\n",
        "        mask = (log_tracks == track_name)\n",
        "        t_subset = log_times[mask]\n",
        "        tau_subset = log_taus[mask]\n",
        "        label_subset = log_labels[mask]\n",
        "\n",
        "        # Color-code based on final identity\n",
        "        colors = ['#1f77b4' if L == 'Doctor' else '#d62728' if L == 'Patient' else 'none' for L in label_subset]\n",
        "\n",
        "        # Only plot if there's an actual assignment (ignores the \"none\" cases)\n",
        "        plt.scatter(t_subset, tau_subset, c=colors, s=35, marker=marker,\n",
        "                    alpha=0.8, edgecolors='none', label=f'Decision: {track_name}')\n",
        "\n",
        "    # --- STYLING & CONTEXT ---\n",
        "    plt.axhline(0, color='black', linestyle='--', alpha=0.2)\n",
        "    plt.ylim(-35, 35)\n",
        "    plt.xlim(start_sec, end_sec)\n",
        "\n",
        "    plt.title(f\"Forensic Audit: AI Clusters vs. Stitcher Decisions ({start_sec}s - {end_sec}s)\", fontsize=16)\n",
        "    plt.ylabel(\"Sample Delay (Tau)\")\n",
        "    plt.xlabel(\"Time (Seconds)\")\n",
        "\n",
        "    # Custom Forensic Legend\n",
        "    legend_elements = [\n",
        "        Line2D([0], [0], marker='.', color='gray', label='Raw Clusters (AI Hits)', linestyle='None', markersize=10, alpha=0.4),\n",
        "        Line2D([0], [0], marker='o', color='w', label='Track A (MossFormer)', markerfacecolor='black', markersize=8),\n",
        "        Line2D([0], [0], marker='x', color='w', label='Track B (MossFormer)', markeredgecolor='black', markersize=8),\n",
        "        Line2D([0], [0], color='#1f77b4', lw=4, label='Doctor (Final Assignment)'),\n",
        "        Line2D([0], [0], color='#d62728', lw=4, label='Patient (Final Assignment)')\n",
        "    ]\n",
        "    plt.legend(handles=legend_elements, loc='upper right', frameon=True, shadow=True)\n",
        "\n",
        "    plt.grid(True, alpha=0.1)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# --- EXECUTE ---\n",
        "# plot_live_decisions(0, 250, warm_results, forensic_log)"
      ],
      "metadata": {
        "id": "5jzCGr1LtqXG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def harvest_ai_clusters(ai_track, other_mic):\n",
        "        # We use the 'Ultra-Sensitive' cluster finder you liked\n",
        "        _, dense_pts = find_dense_clusters(ai_track, other_mic, sr=16000)\n",
        "        return dense_pts\n",
        "track_a = m1_sep[0].squeeze()\n",
        "track_b = m1_sep[1].squeeze()\n",
        "\n",
        "# 2. HARVEST CLUSTERS FROM AI TRACKS\n",
        "# We probe both tracks to find where the voices are sitting spatially\n",
        "print(\"üîé Probing AI Track A for clusters...\")\n",
        "clusters_t0 = harvest_ai_clusters(track_a, patient_audio) # Compares AI output to Raw Patient Mic\n",
        "\n",
        "print(\"üîé Probing AI Track B for clusters...\")\n",
        "clusters_t1 = harvest_ai_clusters(track_b, patient_audio)\n",
        "\n",
        "# 3. RUN THE FORENSIC STITCHER\n",
        "# This generates the audio AND the internal log of every decision made\n",
        "finaldoc, finalpat, forensic_log = event_based_stitcher_with_forensics(\n",
        "    warm_results,\n",
        "    clusters_t0,\n",
        "    clusters_t1,\n",
        "    track_a,\n",
        "    track_b\n",
        ")\n",
        "\n",
        "# 4. PLOT THE LIVE DECISIONS\n",
        "plot_live_decisions(0, 250, warm_results, forensic_log)\n",
        "\n",
        "# 5. LISTEN TO THE RESULTS\n",
        "from IPython.display import Audio, display\n",
        "print(\"üîä Final Doctor Result:\")\n",
        "display(Audio(finaldoc, rate=16000))\n",
        "print(\"üîä Final Patient Result:\")\n",
        "display(Audio(finalpat, rate=16000))"
      ],
      "metadata": {
        "id": "fePA7ZfE0DjM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming m1_sep and m2_sep are already generated from the previous code\n",
        "print(\"--- RAW MOSSFORMER OUTPUTS (MIC 1) ---\")\n",
        "print(\"These are the AI's 'Best Guess' before the Radar sorts them.\")\n",
        "display(Audio(m1_sep[0].squeeze(), rate=16000))\n",
        "display(Audio(m1_sep[1].squeeze(), rate=16000))"
      ],
      "metadata": {
        "id": "uhf3p7ZISEhN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def true_forensic_audit(start_s, end_s, radar_truth, clusters_t0, clusters_t1, f_doc, f_pat, sr=16000):\n",
        "    \"\"\"\n",
        "    Shows which AI dots actually resulted in audio in the final files.\n",
        "    \"\"\"\n",
        "    r_win = radar_truth[(radar_truth[:, 0].astype(float) >= start_s) & (radar_truth[:, 0].astype(float) <= end_s)]\n",
        "\n",
        "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(18, 14), sharex=True)\n",
        "\n",
        "    # PANEL 1: DOCTOR FILE AUDIT\n",
        "    # We only plot dots from Track A/B IF they actually exist in the final_doctor audio\n",
        "    ax1.plot(r_win[:, 0].astype(float), r_win[:, 2].astype(float), color='blue', lw=2, label=\"Doctor Lane\")\n",
        "\n",
        "    for t, tau in clusters_t0:\n",
        "        if start_s <= t <= end_s:\n",
        "            # Check if there is actual audio in final_doctor at this time\n",
        "            idx = int(t * sr)\n",
        "            if idx < len(f_doc) and np.abs(f_doc[idx:idx+640]).max() > 0.005:\n",
        "                ax1.scatter(t, tau, color='blue', s=15)\n",
        "\n",
        "    ax1.set_title(\"WHAT IS ACTUALLY IN THE DOCTOR FILE (Dots = Audio Present)\", fontweight='bold')\n",
        "    ax1.set_ylim(-30, 30); ax1.grid(True, alpha=0.1)\n",
        "\n",
        "    # PANEL 2: PATIENT FILE AUDIT\n",
        "    ax2.plot(r_win[:, 0].astype(float), r_win[:, 3].astype(float), color='red', lw=2, label=\"Patient Lane\")\n",
        "\n",
        "    for t, tau in clusters_t1:\n",
        "        if start_s <= t <= end_s:\n",
        "            idx = int(t * sr)\n",
        "            if idx < len(f_pat) and np.abs(f_pat[idx:idx+640]).max() > 0.005:\n",
        "                ax2.scatter(t, tau, color='red', s=15)\n",
        "\n",
        "    ax2.set_title(\"WHAT IS ACTUALLY IN THE PATIENT FILE\", fontweight='bold')\n",
        "    ax2.set_ylim(-30, 30); ax2.grid(True, alpha=0.1)\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# Run this to see the TRUTH\n",
        "true_forensic_audit(0, 250, warm_results, clusters_t0, clusters_t1, finaldoc, finalpat)"
      ],
      "metadata": {
        "id": "D_CebzfatSWk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import gc\n",
        "import torch\n",
        "import librosa\n",
        "import soundfile as sf\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from clearvoice import ClearVoice\n",
        "# my saving method to run everything and save stuff\n",
        "# --- 1. CONFIGURATION & PATHS ---\n",
        "base_path = \"/content/drive/My Drive/GEEKED/Audio_Files/\"\n",
        "output_path = os.path.join(base_path, \"Final_Outputs\")\n",
        "os.makedirs(output_path, exist_ok=True)\n",
        "\n",
        "sessions = [f\"Session_{str(i).zfill(3)}\" for i in range(1, 11)]\n",
        "\n",
        "\n",
        "def run_end_to_end_pipeline(session_list):\n",
        "    print(\"üì¶ Initializing MossFormer2 Engine...\")\n",
        "    mv_engine = ClearVoice(task='speech_separation', model_names=['MossFormer2_SS_16K'])\n",
        "\n",
        "    # MANUAL GPU OVERDRIVE: Force weights to T4\n",
        "    if torch.cuda.is_available():\n",
        "        try:\n",
        "            if hasattr(mv_engine, 'model'): mv_engine.model.cuda()\n",
        "            elif hasattr(mv_engine, 'network'): mv_engine.network.cuda()\n",
        "            print(\"‚úÖ CUDA Overdrive Active: Model pushed to T4.\")\n",
        "        except: print(\"‚ö†Ô∏è Manual GPU push failed. Using default device.\")\n",
        "\n",
        "    for session_id in tqdm(session_list, desc=\"Processing Sessions\"):\n",
        "        session_folder = os.path.join(base_path, session_id)\n",
        "        raw_doc_mic = os.path.join(session_folder, \"doc_clean.wav\")\n",
        "        raw_pat_mic = os.path.join(session_folder, \"pat_clean.wav\")\n",
        "\n",
        "        if not os.path.exists(raw_doc_mic): continue\n",
        "\n",
        "        try:\n",
        "            # STEP A: GPU INFERENCE\n",
        "            print(f\"\\nüéß GPU Separation: {session_id}\")\n",
        "            with torch.no_grad():\n",
        "                with torch.cuda.amp.autocast(): # Mixed precision for T4 speed\n",
        "                    m1_sep = mv_engine(input_path=raw_doc_mic, online_write=False)\n",
        "\n",
        "            track_a = m1_sep[0].squeeze()\n",
        "            track_b = m1_sep[1].squeeze()\n",
        "\n",
        "            # STEP B: RADAR & CLUSTERING\n",
        "            d_mic, _ = librosa.load(raw_doc_mic, sr=16000)\n",
        "            p_mic, _ = librosa.load(raw_pat_mic, sr=16000)\n",
        "            min_len = min(len(d_mic), len(p_mic), len(track_a), len(track_b))\n",
        "\n",
        "            _, raw_dense = find_dense_clusters(d_mic[:min_len], p_mic[:min_len])\n",
        "            radar_master_key = run_ultimate_robust_radar(raw_dense)\n",
        "\n",
        "            clusters_t0 = harvest_ai_clusters(track_a[:min_len], p_mic[:min_len])\n",
        "            clusters_t1 = harvest_ai_clusters(track_b[:min_len], p_mic[:min_len])\n",
        "\n",
        "            # STEP C: STITCHING (L1 vs L2)\n",
        "            l1_audio, l2_audio, forensic_log = event_based_stitcher_with_forensics(\n",
        "                radar_master_key, clusters_t0, clusters_t1, track_a[:min_len], track_b[:min_len]\n",
        "            )\n",
        "\n",
        "            # STEP D: TALK-TIME IDENTITY (Who is the Doctor?)\n",
        "            l1_count = np.sum(forensic_log[:, 3] == \"L1\")\n",
        "            l2_count = np.sum(forensic_log[:, 3] == \"L2\")\n",
        "\n",
        "            if l1_count >= l2_count:\n",
        "                final_doc, final_pat = l1_audio, l2_audio\n",
        "                print(f\"‚öñÔ∏è Result: L1 (Doctor) spoke more ({l1_count} windows)\")\n",
        "            else:\n",
        "                final_doc, final_pat = l2_audio, l1_audio\n",
        "                print(f\"‚öñÔ∏è Result: L2 (Doctor) spoke more ({l2_count} windows)\")\n",
        "\n",
        "            # STEP E: SAVE\n",
        "\n",
        "            sf.write(os.path.join(output_path, f\"{session_id}_doctor.wav\"), final_doc, 16000)\n",
        "            sf.write(os.path.join(output_path, f\"{session_id}_patient.wav\"), final_pat, 16000)\n",
        "\n",
        "        except Exception as e: print(f\"‚ùå Error in {session_id}: {e}\")\n",
        "        finally:\n",
        "            if torch.cuda.is_available(): torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "\n",
        "# --- RUN ---\n",
        "run_end_to_end_pipeline(sessions)"
      ],
      "metadata": {
        "id": "5cH32k-Ee1vH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def visualize_new_logic_clusters(start_sec, end_sec, radar_truth, clusters_t0, clusters_t1, track_a, track_b, f_doc, f_pat, sr=16000):\n",
        "    \"\"\"\n",
        "    Visualizes how the new Zero-Latency logic handles clusters and routing.\n",
        "    \"\"\"\n",
        "    s_idx, e_idx = int(start_sec * sr), int(end_sec * sr)\n",
        "    time_axis = np.linspace(start_sec, end_sec, e_idx - s_idx)\n",
        "\n",
        "    # Filter Clusters\n",
        "    window_c0 = np.array([pt for pt in clusters_t0 if start_sec <= pt[0] <= end_sec])\n",
        "    window_c1 = np.array([pt for pt in clusters_t1 if start_sec <= pt[0] <= end_sec])\n",
        "\n",
        "    # Global Lane Anchors\n",
        "    doc_pts = [r[1] for r in radar_truth if r[4] == \"Doctor\"]\n",
        "    pat_pts = [r[1] for r in radar_truth if r[4] == \"Patient\"]\n",
        "    doc_lane = np.median(doc_pts) if doc_pts else 10.0\n",
        "    pat_lane = np.median(pat_pts) if pat_pts else -12.0\n",
        "\n",
        "    fig, axs = plt.subplots(3, 1, figsize=(15, 12), sharex=True)\n",
        "\n",
        "    # 1. TOP: SPATIAL MAPPING (Are clusters lining up?)\n",
        "    if len(window_c0) > 0:\n",
        "        axs[0].scatter(window_c0[:, 0], window_c0[:, 1], color='blue', s=25, alpha=0.7, label='Track A (Doc-Track)')\n",
        "    if len(window_c1) > 0:\n",
        "        axs[0].scatter(window_c1[:, 0], window_c1[:, 1], color='red', s=25, alpha=0.7, label='Track B (Pat-Track)')\n",
        "\n",
        "    axs[0].axhline(doc_lane, color='blue', ls='--', alpha=0.4, label='Doc Chair')\n",
        "    axs[0].axhline(pat_lane, color='red', ls='--', alpha=0.4, label='Pat Chair')\n",
        "    axs[0].set_title(\"Phase Clusters (Geometry Check)\", fontsize=14, fontweight='bold')\n",
        "    axs[0].legend(loc='upper right')\n",
        "    axs[0].grid(True, alpha=0.2)\n",
        "\n",
        "    # 2. MIDDLE: VOLUME ENVELOPES (Original vs Result)\n",
        "    axs[1].plot(time_axis, track_a[s_idx:e_idx], color='blue', alpha=0.3, label='Raw Track A')\n",
        "    axs[1].plot(time_axis, f_doc[s_idx:e_idx], color='blue', lw=1.5, label='Cleaned Doctor Output')\n",
        "    axs[1].set_title(\"Audio Volume & Gating Check\", fontsize=14, fontweight='bold')\n",
        "    axs[1].legend(loc='upper right')\n",
        "\n",
        "    # 3. BOTTOM: THE LOGIC TRACE (Why did it mute?)\n",
        "    # We create a mask where the final output is zero but the raw track had audio\n",
        "    doc_muted = (np.abs(track_a[s_idx:e_idx]) > 0.005) & (f_doc[s_idx:e_idx] == 0)\n",
        "    pat_muted = (np.abs(track_b[s_idx:e_idx]) > 0.005) & (f_pat[s_idx:e_idx] == 0)\n",
        "\n",
        "    axs[2].fill_between(time_axis, 0, 1, where=doc_muted, color='blue', alpha=0.4, label='Track A Muted (Leakage Detected)')\n",
        "    axs[2].fill_between(time_axis, -1, 0, where=pat_muted, color='red', alpha=0.4, label='Track B Muted (Leakage Detected)')\n",
        "    axs[2].axhline(0, color='black', lw=1)\n",
        "    axs[2].set_title(\"Logic Decisions: Muted vs Kept\", fontsize=14, fontweight='bold')\n",
        "    axs[2].set_xlabel(\"Time (Seconds)\")\n",
        "    axs[2].set_yticks([])\n",
        "    axs[2].legend(loc='upper right')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# RUN IT on a known swap or 'mhm' area\n",
        "visualize_new_logic_clusters(0, 120, radar_master_key, clusters_t0, clusters_t1, track_a, track_b, hybrid_doc, hybrid_pat)\n"
      ],
      "metadata": {
        "id": "NKKuQHENU8op"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def overlay_ai_clusters(original_dense, ai_track_0, ai_track_1, mic2_audio, sr=16000):\n",
        "    # 1. RUN CLUSTER FINDING on the AI tracks\n",
        "    def harvest_ai_clusters(ai_track, other_mic):\n",
        "        # We use the 'Ultra-Sensitive' cluster finder you liked\n",
        "        _, dense_pts = find_dense_clusters(ai_track, other_mic, sr=sr)\n",
        "        return dense_pts\n",
        "\n",
        "    print(\"Finding clusters in AI Track 0...\")\n",
        "    clusters_t0 = harvest_ai_clusters(ai_track_0, mic2_audio)\n",
        "\n",
        "    print(\"Finding clusters in AI Track 1...\")\n",
        "    clusters_t1 = harvest_ai_clusters(ai_track_1, mic2_audio)\n",
        "\n",
        "    # 2. PLOTTING\n",
        "    plt.figure(figsize=(16, 12))\n",
        "\n",
        "    # --- TOP: AI Track 0 ---\n",
        "    plt.subplot(2, 1, 1)\n",
        "    # Background (Ground Truth)\n",
        "    plt.scatter(original_dense[:, 0], original_dense[:, 1], color='lightgrey', s=3, alpha=0.2, label=\"Original Ground Truth\")\n",
        "    # AI Clusters (What the AI actually kept)\n",
        "    if len(clusters_t0) > 0:\n",
        "        plt.scatter(clusters_t0[:, 0], clusters_t0[:, 1], color='blue', s=8, alpha=0.7, label=\"AI Clusters (Track 0)\")\n",
        "    plt.axhline(10, color='red', linestyle='--', alpha=0.3)\n",
        "    plt.axhline(-12, color='black', linestyle='--', alpha=0.3)\n",
        "    plt.title(\"Track 0: AI Clusters (Blue) vs. Ground Truth (Grey)\")\n",
        "    plt.legend()\n",
        "\n",
        "    # --- BOTTOM: AI Track 1 ---\n",
        "    plt.subplot(2, 1, 2)\n",
        "    # Background (Ground Truth)\n",
        "    plt.scatter(original_dense[:, 0], original_dense[:, 1], color='lightgrey', s=3, alpha=0.2, label=\"Original Ground Truth\")\n",
        "    # AI Clusters\n",
        "    if len(clusters_t1) > 0:\n",
        "        plt.scatter(clusters_t1[:, 0], clusters_t1[:, 1], color='red', s=8, alpha=0.7, label=\"AI Clusters (Track 1)\")\n",
        "    plt.axhline(10, color='red', linestyle='--', alpha=0.3)\n",
        "    plt.axhline(-12, color='black', linestyle='--', alpha=0.3)\n",
        "    plt.title(\"Track 1: AI Clusters (Red) vs. Ground Truth (Grey)\")\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "track_a = m1_sep[0].squeeze()\n",
        "track_b = m1_sep[1].squeeze()\n",
        "# --- EXECUTE ---\n",
        "# Use 'raw_dense' (your perfect ground truth) and the 'ai_tr0'/'ai_tr1' from MossFormer\n",
        "overlay_ai_clusters(raw_dense, track_a, track_b, patient_audio)"
      ],
      "metadata": {
        "id": "MljceiUUeMfB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def visualize_final_stitched_clusters(raw_dense, final_doc_audio, final_pat_audio, ref_mic_audio, sr=16000):\n",
        "    print(\"Harvesting spatial clusters for the FINAL stitched files... (this might take a second)\")\n",
        "\n",
        "    # 1. Run your cluster finder on the newly stitched tracks\n",
        "    # We compare the final separated tracks against the original reference mic\n",
        "    _, doc_clusters = find_dense_clusters(final_doc_audio, ref_mic_audio, sr=sr)\n",
        "    _, pat_clusters = find_dense_clusters(final_pat_audio, ref_mic_audio, sr=sr)\n",
        "\n",
        "    # 2. PLOTTING\n",
        "    plt.figure(figsize=(16, 12))\n",
        "\n",
        "    # --- TOP: Final Doctor Track ---\n",
        "    plt.subplot(2, 1, 1)\n",
        "    # Background Ghost Map\n",
        "    plt.scatter(raw_dense[:, 0], raw_dense[:, 1], color='lightgrey', s=3, alpha=0.2, label=\"Original Ground Truth\")\n",
        "\n",
        "    # Stitched Doctor Clusters\n",
        "    if len(doc_clusters) > 0:\n",
        "        plt.scatter(doc_clusters[:, 0], doc_clusters[:, 1], color='blue', s=8, alpha=0.7, label=\"Final Doctor Track\")\n",
        "\n",
        "    plt.axhline(10, color='blue', linestyle='--', alpha=0.3, label=\"Expected Doctor Lane\")\n",
        "    plt.axhline(-12, color='red', linestyle='--', alpha=0.3, label=\"Expected Patient Lane\")\n",
        "    plt.title(\"FINAL STITCHED: Doctor Track (Should ONLY hover near top line)\", fontsize=14, fontweight='bold')\n",
        "    plt.ylabel(\"Phase Shift (tau)\")\n",
        "    plt.legend()\n",
        "\n",
        "    # --- BOTTOM: Final Patient Track ---\n",
        "    plt.subplot(2, 1, 2)\n",
        "    # Background Ghost Map\n",
        "    plt.scatter(raw_dense[:, 0], raw_dense[:, 1], color='lightgrey', s=3, alpha=0.2, label=\"Original Ground Truth\")\n",
        "\n",
        "    # Stitched Patient Clusters\n",
        "    if len(pat_clusters) > 0:\n",
        "        plt.scatter(pat_clusters[:, 0], pat_clusters[:, 1], color='red', s=8, alpha=0.7, label=\"Final Patient Track\")\n",
        "\n",
        "    plt.axhline(10, color='blue', linestyle='--', alpha=0.3)\n",
        "    plt.axhline(-12, color='red', linestyle='--', alpha=0.3)\n",
        "    plt.title(\"FINAL STITCHED: Patient Track (Should ONLY hover near bottom line)\", fontsize=14, fontweight='bold')\n",
        "    plt.ylabel(\"Phase Shift (tau)\")\n",
        "    plt.xlabel(\"Time (seconds)\")\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# --- EXECUTE IT ---\n",
        "# finaldoc and finalpat are the outputs from our stitching function\n",
        "# patient_audio is your raw reference mic from earlier\n",
        "visualize_final_stitched_clusters(raw_dense, finaldoc, finalpat, patient_audio)"
      ],
      "metadata": {
        "id": "RNwDcMkhwgqS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}